{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47dbf70-b50a-43fe-94e3-8d3bb620c281",
   "metadata": {},
   "source": [
    "# ì—°ìŠµë¬¸ì œ: í•œ ë²ˆì— í•˜ë‚˜ì˜ í† í° ìƒì„±í•˜ê¸°\n",
    "\n",
    "ì´ ì—°ìŠµì—ì„œëŠ” LLMì´ ì´ì „ í† í°ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ìŒ í† í°ì„ ì˜ˆì¸¡í•˜ë©´ì„œ í•œ ë²ˆì— í•˜ë‚˜ì˜ í† í°ì”© í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ì„ ì´í•´í•˜ê²Œ ë  ê²ƒì…ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c731d9-a5b5-4f71-9463-0952b17e3c1d",
   "metadata": {},
   "source": [
    "## 1ë‹¨ê³„. í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œí•˜ê¸°\n",
    "\n",
    "ë¨¼ì € HuggingFaceì˜ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤. í† í¬ë‚˜ì´ì €ëŠ” ëª¨ë¸ì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ìˆ«ì ëª©ë¡ìœ¼ë¡œ ë¬¸ìì—´ì„ ë¶„í• í•˜ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n",
    "\n",
    "ì´ ì—°ìŠµì—ì„œëŠ” ëª¨ë“  ì½”ë“œê°€ ì—¬ëŸ¬ë¶„ì„ ìœ„í•´ ì‘ì„±ë  ê²ƒì…ë‹ˆë‹¤. ì—¬ëŸ¬ë¶„ì€ ë”°ë¼í•˜ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46d0c56",
   "metadata": {},
   "source": [
    "### ì„¤ì¹˜í•´ì•¼ í•  ëª©ë¡\n",
    "- python3.11 -m venv .venv\n",
    "- source .venv/bin/activate\n",
    "- pip install --upgrade pip\n",
    "- pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "- pip install transformers\n",
    "- pip install pandas\n",
    "- pip install IPython\n",
    "- pip install jupyterlab ipywidgets\n",
    "- jupyter lab build\n",
    "\n",
    "### ì„¤ì¹˜í›„ ì‹¤í–‰\n",
    "- jupyter lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f618acdd-06a3-4d3c-981d-fb11632f97ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  52,   67, 4355,  318,  262, 1266, 1295,  284, 2193,  546, 1152,  876]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# To load a pretrained model and a tokenizer using HuggingFace, we only need two lines of code!\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# We create a partial sentence and tokenize it.\n",
    "text = \"Udacity is the best place to learn about generative\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Show the tokens as numbers, i.e. \"input_ids\"\n",
    "inputs[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66625be-e6eb-46af-988c-d313f3a24815",
   "metadata": {},
   "source": [
    "## 2ë‹¨ê³„. í† í°í™” ê²€í† í•˜ê¸°\n",
    "\n",
    "ì´ í† í°ë“¤ì´ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ëŠ”ì§€ ì‚´í´ë´…ì‹œë‹¤!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "55d4c411-eb20-4e5f-a872-18536da50e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tensor(52)</td>\n",
       "      <td>U</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>tensor(67)</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>tensor(4355)</td>\n",
       "      <td>acity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tensor(318)</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>tensor(262)</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>tensor(1266)</td>\n",
       "      <td>best</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tensor(1295)</td>\n",
       "      <td>place</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tensor(284)</td>\n",
       "      <td>to</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tensor(2193)</td>\n",
       "      <td>learn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tensor(546)</td>\n",
       "      <td>about</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tensor(1152)</td>\n",
       "      <td>gener</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>tensor(876)</td>\n",
       "      <td>ative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              id   token\n",
       "0     tensor(52)       U\n",
       "1     tensor(67)       d\n",
       "2   tensor(4355)   acity\n",
       "3    tensor(318)      is\n",
       "4    tensor(262)     the\n",
       "5   tensor(1266)    best\n",
       "6   tensor(1295)   place\n",
       "7    tensor(284)      to\n",
       "8   tensor(2193)   learn\n",
       "9    tensor(546)   about\n",
       "10  tensor(1152)   gener\n",
       "11   tensor(876)   ative"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show how the sentence is tokenized\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def show_tokenization(inputs):\n",
    "    return pd.DataFrame(\n",
    "        [(id, tokenizer.decode(id)) for id in inputs[\"input_ids\"][0]],\n",
    "        columns=[\"id\", \"token\"],\n",
    "    )\n",
    "\n",
    "\n",
    "show_tokenization(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4c1b5e1-ef04-4a11-9ed4-ce920564b2ef",
   "metadata": {},
   "source": [
    "### ì„œë¸Œì›Œë“œ í† í°í™”\n",
    "\n",
    "í¥ë¯¸ë¡œìš´ ì ì€ ì´ ê²½ìš° í† í°ì´ ë‹¨ìˆœíˆ ê¸€ìë„ ì•„ë‹ˆê³  ë‹¨ì–´ë„ ì•„ë‹ˆë¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ë•Œë¡œëŠ” ì§§ì€ ë‹¨ì–´ê°€ ë‹¨ì¼ í† í°ìœ¼ë¡œ í‘œí˜„ë˜ì§€ë§Œ, ë•Œë¡œëŠ” ë‹¨ì¼ í† í°ì´ ë‹¨ì–´ì˜ ì¼ë¶€ ë˜ëŠ” ë‹¨ì¼ ê¸€ìë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ë¥¼ ì„œë¸Œì›Œë“œ í† í°í™”ë¼ê³  í•©ë‹ˆë‹¤.\n",
    "\n",
    "## 2ë‹¨ê³„. ë‹¤ìŒ í† í°ì˜ í™•ë¥  ê³„ì‚°í•˜ê¸°\n",
    "\n",
    "ì´ì œ PyTorchë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ì „ í† í°ì´ ì£¼ì–´ì¡Œì„ ë•Œ ë‹¤ìŒ í† í°ì˜ í™•ë¥ ì„ ê³„ì‚°í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8bd74f6-a363-4aef-b9c9-113e7d569afc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>8300</td>\n",
       "      <td>programming</td>\n",
       "      <td>0.157596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4673</th>\n",
       "      <td>4673</td>\n",
       "      <td>learning</td>\n",
       "      <td>0.148418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4981</th>\n",
       "      <td>4981</td>\n",
       "      <td>models</td>\n",
       "      <td>0.048506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17219</th>\n",
       "      <td>17219</td>\n",
       "      <td>biology</td>\n",
       "      <td>0.046482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16113</th>\n",
       "      <td>16113</td>\n",
       "      <td>algorithms</td>\n",
       "      <td>0.027796</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id         token         p\n",
       "8300    8300   programming  0.157596\n",
       "4673    4673      learning  0.148418\n",
       "4981    4981        models  0.048506\n",
       "17219  17219       biology  0.046482\n",
       "16113  16113    algorithms  0.027796"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the probabilities for the next token for all possible choices. We show the\n",
    "# top 5 choices and the corresponding words or subwords for these tokens.\n",
    "\n",
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "\n",
    "\n",
    "def show_next_token_choices(probabilities, top_n=5):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            (id, tokenizer.decode(id), p.item())\n",
    "            for id, p in enumerate(probabilities)\n",
    "            if p.item()\n",
    "        ],\n",
    "        columns=[\"id\", \"token\", \"p\"],\n",
    "    ).sort_values(\"p\", ascending=False)[:top_n]\n",
    "\n",
    "\n",
    "show_next_token_choices(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf754b52-0841-4c07-9f65-e76edffed21c",
   "metadata": {},
   "source": [
    "í¥ë¯¸ë¡­ë„¤ìš”! ëª¨ë¸ì€ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë‹¤ìŒ ë‹¨ì–´ê°€ \"programming\"ì´ê³ , ê·¸ ë‹¤ìŒìœ¼ë¡œ \"learning\"ì´ ë’¤ë”°ë¥¸ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f56c9d5-8b3e-4f66-bc05-4d2d85f02558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next token id: 8300\n",
      "Next token:  programming\n"
     ]
    }
   ],
   "source": [
    "# Obtain the token id for the most probable next token\n",
    "next_token_id = torch.argmax(probabilities).item()\n",
    "\n",
    "print(f\"Next token id: {next_token_id}\")\n",
    "print(f\"Next token: {tokenizer.decode(next_token_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "628ad7e1-9c6a-44a7-a170-22e548edfd68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Udacity is the best place to learn about generative programming'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We append the most likely token to the text.\n",
    "text = text + tokenizer.decode(8300)\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30d5f3e-0729-4d9d-821d-f70dc14eada1",
   "metadata": {},
   "source": [
    "## 3ë‹¨ê³„. í† í°ì„ ë” ìƒì„±í•˜ê¸°\n",
    "\n",
    "ë‹¤ìŒ ì…€ì€ `text`ë¥¼ ê°€ì ¸ì™€ì„œ ë‹¤ìŒì— ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ì€ í† í°ì„ í‘œì‹œí•˜ê³ , ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ í† í°ì„ textì— ì¶”ê°€í•©ë‹ˆë‹¤. ì…€ì„ ë°˜ë³µí•´ì„œ ì‹¤í–‰í•˜ì—¬ ì‘ë™ ë°©ì‹ì„ í™•ì¸í•˜ì„¸ìš”!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c221cb21-d0a9-4f0e-889d-e28c0eab0e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Udacity is the best place to learn about generative programming\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Next token probabilities:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>token</th>\n",
       "      <th>p</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>.</td>\n",
       "      <td>0.352234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>,</td>\n",
       "      <td>0.135988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>290</td>\n",
       "      <td>and</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>287</td>\n",
       "      <td>in</td>\n",
       "      <td>0.069531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8950</th>\n",
       "      <td>8950</td>\n",
       "      <td>languages</td>\n",
       "      <td>0.058290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id       token         p\n",
       "13      13           .  0.352234\n",
       "11      11           ,  0.135988\n",
       "290    290         and  0.109375\n",
       "287    287          in  0.069531\n",
       "8950  8950   languages  0.058290"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Press ctrl + enter to run this cell again and again to see how the text is generated.\n",
    "\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Show the text\n",
    "print(text)\n",
    "\n",
    "# Convert to tokens\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Calculate the probabilities for the next token and show the top 5 choices\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits[:, -1, :]\n",
    "    probabilities = torch.nn.functional.softmax(logits[0], dim=-1)\n",
    "\n",
    "display(Markdown(\"**Next token probabilities:**\"))\n",
    "display(show_next_token_choices(probabilities))\n",
    "\n",
    "# Choose the most likely token id and add it to the text\n",
    "next_token_id = torch.argmax(probabilities).item()\n",
    "text = text + tokenizer.decode(next_token_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c94599-172a-4945-acb1-6107e1397c8b",
   "metadata": {},
   "source": [
    "## Step 4. Use the `generate` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df9808b5-7305-4f4d-bba2-423970512e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Once upon a time, generative models of the human brain were used to study the neural correlates of cognitive function. In the present study, we used a novel model of the human brain to investigate the neural correlates of cognitive function. We used a novel model of the human brain to investigate the neural correlates of cognitive function. We used a novel model of the human brain to investigate the neural correlates of cognitive function. We used a novel model of the human brain to investigate the neural correlates of cognitive function."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "# Start with some text and tokenize it\n",
    "text = \"Once upon a time, generative models\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "# Use the `generate` method to generate lots of text\n",
    "output = model.generate(**inputs, max_length=100, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "# Show the generated text\n",
    "display(Markdown(tokenizer.decode(output[0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0ec2eb-c0f7-4f8e-918d-519a08dbd62e",
   "metadata": {},
   "source": [
    "ì˜›ë‚  ì˜›ì ì— ì¸ê°„ ë‘ë‡Œì˜ ìƒì„± ëª¨ë¸ì€ ì¸ì§€ ê¸°ëŠ¥ì˜ ì‹ ê²½ ìƒê´€ ê´€ê³„ë¥¼ ì—°êµ¬í•˜ëŠ” ë° ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ì—ì„œëŠ” ì¸ê°„ ë‘ë‡Œì˜ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¸ì§€ ê¸°ëŠ¥ì˜ ì‹ ê²½ ìƒê´€ ê´€ê³„ë¥¼ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì¸ê°„ ë‘ë‡Œì˜ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¸ì§€ ê¸°ëŠ¥ì˜ ì‹ ê²½ ìƒê´€ ê´€ê³„ë¥¼ ì¡°ì‚¬í–ˆìŠµë‹ˆë‹¤.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f82553c-318e-4f1c-a716-e52ba70f2ef3",
   "metadata": {},
   "source": [
    "### í¥ë¯¸ë¡­ë„¤ìš”...\n",
    "\n",
    "GPT-2ëŠ” ì—¬ëŸ¬ë¶„ì´ ì‚¬ìš© ê²½í—˜ì´ ìˆì„ ìˆ˜ ìˆëŠ” GPT-4ì™€ ê°™ì€ ìµœì‹  ëª¨ë¸ë§Œí¼ ì •êµí•˜ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ì¢…ì¢… ìŠ¤ìŠ¤ë¡œ ë°˜ë³µí•˜ê³  í•­ìƒ ë§ì´ ë˜ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. í•˜ì§€ë§Œ ì˜ì–´ì²˜ëŸ¼ ë³´ì´ëŠ” í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì€ ì—¬ì „íˆ ë§¤ìš° ì¸ìƒì ì…ë‹ˆë‹¤.\n",
    "\n",
    "## ì—°ìŠµë¬¸ì œ ì™„ë£Œë¥¼ ì¶•í•˜í•©ë‹ˆë‹¤! ğŸ‰\n",
    "\n",
    "ìŠ¤ìŠ¤ë¡œì—ê²Œ ë°•ìˆ˜ë¥¼ ë³´ë‚´ì„¸ìš”. ê·¸ë¦¬ê³  íœ´ì‹ì„ ì·¨í•˜ì„¸ìš”.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49b1dd4-2e5a-4042-b83c-2256672d79fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
